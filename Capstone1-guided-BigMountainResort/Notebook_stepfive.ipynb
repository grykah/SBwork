{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf5CmXQCZyF1"
   },
   "source": [
    "# Guided Capstone Step 4. Pre-Processing and Training Data Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2jue2jPGJlt"
   },
   "source": [
    "**The Data Science Method**  \n",
    "\n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "\n",
    "2.   Data Wrangling \n",
    "  \n",
    " \n",
    "3.   Exploratory Data Analysis   \n",
    "\n",
    "4.   **Pre-processing and Training Data Development**  \n",
    " * Create dummy or indicator features for categorical variables\n",
    "  * Standardize the magnitude of numeric features\n",
    "  * Split into testing and training datasets\n",
    "  * Apply scaler to the testing set\n",
    "5.   Modeling \n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes â€” Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8xfkAqqZyF2"
   },
   "source": [
    "**<font color='teal'> Start by loading the necessary packages as we did in step 3 and printing out our current working directory just to confirm we are in the correct project directory. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ry6WPL5eZyF3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hailey\\\\JNote'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load python packages\n",
    "import os #op sys access\n",
    "import pandas as pd\n",
    "import datetime  #date work\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing #preprocessing package\n",
    "from sklearn.model_selection import train_test_split #train_test_split package for modeling\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import explained_variance_score,mean_absolute_error\n",
    "import pandas_profiling as pp\n",
    "\n",
    "#check path\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "630T-ogRZyF8"
   },
   "source": [
    "**<font color='teal'>  Load the csv file you created in step 3, remember it should be saved inside your data subfolder and print the first five rows.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMNbk0u3ZyF9"
   },
   "outputs": [],
   "source": [
    "#change working directory and check what is in it\n",
    "path = 'C:\\\\Users\\\\Hailey\\\\Documents\\\\temp' #working path\n",
    "dataPathIn = 'C:\\\\Users\\\\Hailey\\\\Documents\\\\temp\\\\data\\\\step3_output_reAnalysis.csv' #data path\n",
    "os.chdir(path) \n",
    "path = os.getcwd() \n",
    "os.listdir()\n",
    "\n",
    "#load the data\n",
    "df = pd.read_csv(dataPathIn)\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkBHf9smZyGB"
   },
   "source": [
    "## Create dummy features for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWKHm0NhAnrJ"
   },
   "source": [
    "**<font color='teal'> Create dummy variables for `state`. Add the dummies back to the dataframe and remove the original column for `state`. </font>**\n",
    "\n",
    "A code segment for this is shown in Aiden's article on preprocessing [here](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-pre-processing-and-training-data-development-fd2d75182967). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZqWk8ltZyGZ"
   },
   "outputs": [],
   "source": [
    "dfo=df[['state']] # select object column 'state'\n",
    "df1 = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1) #add dummies to df with names like 'state_...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        state\n",
      "0      Alaska\n",
      "1     Arizona\n",
      "2  California\n",
      "3  California\n",
      "4  California\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 171 entries, 0 to 170\n",
      "Data columns (total 57 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Name                  171 non-null    object \n",
      " 1   summit_elev           171 non-null    int64  \n",
      " 2   vertical_drop         171 non-null    int64  \n",
      " 3   fastQuads             171 non-null    int64  \n",
      " 4   quad                  171 non-null    int64  \n",
      " 5   triple                171 non-null    int64  \n",
      " 6   double                171 non-null    int64  \n",
      " 7   surface               171 non-null    int64  \n",
      " 8   total_chairs          171 non-null    int64  \n",
      " 9   Runs                  171 non-null    int64  \n",
      " 10  TerrainParks          171 non-null    int64  \n",
      " 11  LongestRun_mi         171 non-null    float64\n",
      " 12  SkiableTerrain_ac     171 non-null    float64\n",
      " 13  Snow Making_ac        171 non-null    float64\n",
      " 14  daysOpenLastYear      171 non-null    int64  \n",
      " 15  yearsOpen             171 non-null    int64  \n",
      " 16  averageSnowfall       171 non-null    float64\n",
      " 17  AdultWeekday          171 non-null    float64\n",
      " 18  AdultWeekend          171 non-null    float64\n",
      " 19  projectedDaysOpen     171 non-null    int64  \n",
      " 20  NightSkiing_ac        171 non-null    float64\n",
      " 21  clusters              171 non-null    int64  \n",
      " 22  state_Alaska          171 non-null    uint8  \n",
      " 23  state_Arizona         171 non-null    uint8  \n",
      " 24  state_California      171 non-null    uint8  \n",
      " 25  state_Colorado        171 non-null    uint8  \n",
      " 26  state_Connecticut     171 non-null    uint8  \n",
      " 27  state_Idaho           171 non-null    uint8  \n",
      " 28  state_Illinois        171 non-null    uint8  \n",
      " 29  state_Indiana         171 non-null    uint8  \n",
      " 30  state_Iowa            171 non-null    uint8  \n",
      " 31  state_Maine           171 non-null    uint8  \n",
      " 32  state_Maryland        171 non-null    uint8  \n",
      " 33  state_Massachusetts   171 non-null    uint8  \n",
      " 34  state_Michigan        171 non-null    uint8  \n",
      " 35  state_Minnesota       171 non-null    uint8  \n",
      " 36  state_Missouri        171 non-null    uint8  \n",
      " 37  state_Montana         171 non-null    uint8  \n",
      " 38  state_Nevada          171 non-null    uint8  \n",
      " 39  state_New Hampshire   171 non-null    uint8  \n",
      " 40  state_New Jersey      171 non-null    uint8  \n",
      " 41  state_New Mexico      171 non-null    uint8  \n",
      " 42  state_New York        171 non-null    uint8  \n",
      " 43  state_North Carolina  171 non-null    uint8  \n",
      " 44  state_Ohio            171 non-null    uint8  \n",
      " 45  state_Oregon          171 non-null    uint8  \n",
      " 46  state_Pennsylvania    171 non-null    uint8  \n",
      " 47  state_Rhode Island    171 non-null    uint8  \n",
      " 48  state_South Dakota    171 non-null    uint8  \n",
      " 49  state_Tennessee       171 non-null    uint8  \n",
      " 50  state_Utah            171 non-null    uint8  \n",
      " 51  state_Vermont         171 non-null    uint8  \n",
      " 52  state_Virginia        171 non-null    uint8  \n",
      " 53  state_Washington      171 non-null    uint8  \n",
      " 54  state_West Virginia   171 non-null    uint8  \n",
      " 55  state_Wisconsin       171 non-null    uint8  \n",
      " 56  state_Wyoming         171 non-null    uint8  \n",
      "dtypes: float64(7), int64(14), object(1), uint8(35)\n",
      "memory usage: 35.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dfo.head())\n",
    "print(df1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 171 entries, 0 to 170\n",
      "Data columns (total 22 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Name               171 non-null    object \n",
      " 1   summit_elev        171 non-null    int64  \n",
      " 2   vertical_drop      171 non-null    int64  \n",
      " 3   fastQuads          171 non-null    int64  \n",
      " 4   quad               171 non-null    int64  \n",
      " 5   triple             171 non-null    int64  \n",
      " 6   double             171 non-null    int64  \n",
      " 7   surface            171 non-null    int64  \n",
      " 8   total_chairs       171 non-null    int64  \n",
      " 9   Runs               171 non-null    int64  \n",
      " 10  TerrainParks       171 non-null    int64  \n",
      " 11  LongestRun_mi      171 non-null    float64\n",
      " 12  SkiableTerrain_ac  171 non-null    float64\n",
      " 13  Snow Making_ac     171 non-null    float64\n",
      " 14  daysOpenLastYear   171 non-null    int64  \n",
      " 15  yearsOpen          171 non-null    int64  \n",
      " 16  averageSnowfall    171 non-null    float64\n",
      " 17  AdultWeekday       171 non-null    float64\n",
      " 18  AdultWeekend       171 non-null    float64\n",
      " 19  projectedDaysOpen  171 non-null    int64  \n",
      " 20  NightSkiing_ac     171 non-null    float64\n",
      " 21  clusters           171 non-null    int64  \n",
      "dtypes: float64(7), int64(14), object(1)\n",
      "memory usage: 29.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df2 = df.drop('state', axis = 1)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features for model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 171 entries, 0 to 170\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Name               171 non-null    object \n",
      " 1   vertical_drop      171 non-null    int64  \n",
      " 2   Runs               171 non-null    int64  \n",
      " 3   TerrainParks       171 non-null    int64  \n",
      " 4   LongestRun_mi      171 non-null    float64\n",
      " 5   SkiableTerrain_ac  171 non-null    float64\n",
      " 6   daysOpenLastYear   171 non-null    int64  \n",
      " 7   AdultWeekday       171 non-null    float64\n",
      " 8   AdultWeekend       171 non-null    float64\n",
      " 9   projectedDaysOpen  171 non-null    int64  \n",
      " 10  NightSkiing_ac     171 non-null    float64\n",
      " 11  clusters           171 non-null    int64  \n",
      "dtypes: float64(5), int64(6), object(1)\n",
      "memory usage: 16.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#df3 = df.drop(['state', 'summit_elev', 'trams', 'fastEight', 'fastSixes', 'fastQuads','quad','triple','double','surface','total_chairs', 'Snow Making_ac','yearsOpen','averageSnowfall'],axis = 1)\n",
    "#redefinition of df3 since trams, fastEight, and fastSixes were removed in EDA\n",
    "df3 = df.drop(['state', 'summit_elev', 'fastQuads','quad','triple','double','surface','total_chairs', 'Snow Making_ac','yearsOpen','averageSnowfall'],axis = 1)\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnDVhE1-ZyGF"
   },
   "source": [
    "## Standardize the magnitude of numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW3D-WlDZyGG"
   },
   "source": [
    "**<font color='teal'> Using sklearn preprocessing standardize the scale of the features of the dataframe except the name of the resort which we done't need in the dataframe for modeling, so it can be droppped here as well. Also, we want to hold out our response variable(s) so we can have their true values available for model performance review. Let's set `AdultWeekend` to the y variable as our response for scaling and modeling. Later we will go back and consider the `AdultWeekday`, `dayOpenLastYear`, and `projectedDaysOpen`. For now leave them in the development dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZL-q-KtAYI6"
   },
   "outputs": [],
   "source": [
    "# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df\n",
    "X1 = df1.drop(['Name','AdultWeekend'], axis=1)\n",
    "X2 = df2.drop(['Name','AdultWeekend'], axis=1)\n",
    "X3 = df3.drop(['Name','AdultWeekend'], axis=1)\n",
    "\n",
    "# Declare a response variable, called y, and assign it the AdultWeekend column of the df \n",
    "y = df.loc[:,'AdultWeekend'] # keeping only adult weekend as response variable since adult weekday highly correlated and increasing income  is our main goal\n",
    "#y2 = df.loc[:,'AdultWeekday']\n",
    "#y3 = df1.loc[:, 'daysOpenLastYear']\n",
    "#y4 = df1.loc[:,'projectedDaysOpen']\n",
    "\n",
    "# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \n",
    "scaler1 = preprocessing.StandardScaler().fit(X1)\n",
    "scaler2 = preprocessing.StandardScaler().fit(X2)\n",
    "scaler3 = preprocessing.StandardScaler().fit(X3)\n",
    "\n",
    "# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \n",
    "X1_scaled=scaler1.transform(X1)\n",
    "X2_scaled=scaler2.transform(X2)\n",
    "X3_scaled=scaler3.transform(X3)\n",
    "#X1_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAT8h4_mZyGK"
   },
   "source": [
    "## Split into training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rdS8EGeAnrW"
   },
   "source": [
    "**<font color='teal'> Using sklearn model selection import train_test_split, and create a 75/25 split with the y = `AdultWeekend`. We will start by using the adult weekend ticket price as our response variable for modeling.   Then, we will use other parameters in y.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSkPut0gguds"
   },
   "outputs": [],
   "source": [
    "# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y\n",
    "y = y.ravel()\n",
    "#y2 = y2.ravel()\n",
    "#y3 = y3.ravel()\n",
    "#y4 = y4.ravel()\n",
    "\n",
    "# Call the train_test_split() function with the first two parameters set to X_scaled and y \n",
    "# Declare four variables, X_train, X_test, y_train and y_test separated by commas \n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1_scaled, y, test_size=0.25, random_state=1)\n",
    "#X1_train2, X1_test2, y_train2, y_test2 = train_test_split(X1_scaled, y2, test_size=0.25, random_state=1)\n",
    "#X1_train3, X1_test3, y_train3, y_test3 = train_test_split(X1_scaled, y3, test_size=0.25, random_state=1)\n",
    "#X1_train4, X1_test4, y_train4, y_test4 = train_test_split(X1_scaled, y4, test_size=0.25, random_state=1)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y, test_size=0.25, random_state=1)\n",
    "#X2_train2, X2_test2, y2_train2, y2_test2 = train_test_split(X2_scaled, y2, test_size=0.25, random_state=1)\n",
    "#X2_train3, X2_test3, y2_train3, y2_test3 = train_test_split(X2_scaled, y3, test_size=0.25, random_state=1)\n",
    "#X2_train4, X2_test4, y2_train4, y2_test4 = train_test_split(X2_scaled, y4, test_size=0.25, random_state=1)\n",
    "\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y, test_size=0.25, random_state=1)\n",
    "#X3_train2, X3_test2, y3_train2, y3_test2 = train_test_split(X3_scaled, y2, test_size=0.25, random_state=1)\n",
    "#X3_train3, X3_test3, y3_train3, y3_test3 = train_test_split(X3_scaled, y3, test_size=0.25, random_state=1)\n",
    "#X3_train4, X3_test4, y3_train4, y3_test4 = train_test_split(X3_scaled, y4, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UayqbwkWAnra"
   },
   "source": [
    "Here we start the actual modeling work. First let's fit a multiple linear regression model to predict the `AdultWeekend` price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83fkLldXFCNd"
   },
   "source": [
    "# Guided Capstone Step 5. Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbZXsVevfr9M"
   },
   "source": [
    "This is the fifth step in the Data Science Method. In the previous steps you cleaned and prepared the datasets. Now it's time to get into the most exciting part: modeling! In this exercise, you'll build three different models and compare each model's performance. In the end, you'll choose the best model for demonstrating insights to Big Mountain management.\n",
    "\n",
    "\n",
    "\n",
    "### **The Data Science Method**  \n",
    "\n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "2.   Data Wrangling \n",
    "  \n",
    "3.   Exploratory Data Analysis \n",
    " \n",
    "4.   Pre-processing and Training Data Development\n",
    "\n",
    "5.   **Modeling**\n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes â€” Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_wfsP_-Anra"
   },
   "source": [
    "## Fit Models with a Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoI8S5SwAnrc"
   },
   "source": [
    "**<font color='teal'> Using sklearn, fit the model on your training dataset.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_GFr8sRAnrd"
   },
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW6K7uOPAnre"
   },
   "outputs": [],
   "source": [
    "#all first model set\n",
    "lm = linear_model.LinearRegression()\n",
    "model_x1y = lm.fit(X1_train,y_train)\n",
    "#model_x12y2 = lm.fit(X1_train2,y_train2)\n",
    "#model_x13y3 = lm.fit(X1_train3,y_train3)\n",
    "#model_x14y4 = lm.fit(X1_train4,y_train4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fHqz9-WAnrg"
   },
   "source": [
    "**<font color='teal'> Predict on the testing dataset and score the model performance with the y_test set and the y-pred values. The explained variance is a measure of the variation explained by the model. This is also known as the R-squared value. </font>**\n",
    "\n",
    "Hint: you will have to use the `predict()` method here as it's used in this [DSM article](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-modeling-56b4233cad1b) about modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIo01lFEAnrh"
   },
   "outputs": [],
   "source": [
    "# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test\n",
    "y_pred1 = model_x1y.predict(X1_test)\n",
    "#y_pred12 = model_x12y2.predict(X1_test2)\n",
    "#y_pred13 = model_x13y3.predict(X1_test3)\n",
    "#y_pred14 = model_x14y4.predict(X1_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4YS0WE2Anrk"
   },
   "source": [
    "## Review Model Outcomes â€” Iterate over additional models as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSh9sGIYAnrk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7610654709802023\n"
     ]
    }
   ],
   "source": [
    "# You might want to use the explained_variance_score() and mean_absolute_error() metrics.\n",
    "# To do so, you will need to import them from sklearn.metrics. \n",
    "# You can plug y_test and y_pred into the functions to evaluate the model\n",
    "print(explained_variance_score(y_test, y_pred1))\n",
    "#print(explained_variance_score(y_test2, y_pred12))\n",
    "#print(explained_variance_score(y_test3, y_pred13))\n",
    "#print(explained_variance_score(y_test4, y_pred14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihzeo8tqAnro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.982069319413186\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred1))\n",
    "#print(mean_absolute_error(y_test2, y_pred12))\n",
    "#print(mean_absolute_error(y_test3, y_pred13))\n",
    "#print(mean_absolute_error(y_test4, y_pred14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWJcOuSdAnrr"
   },
   "source": [
    "**<font color='teal'> Print the intercept value from the linear model. </font>**\n",
    "\n",
    "Hint: our linear regression model `lm` has an attribute `intercept_` for the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WzWejn6Anrt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.72298555107527"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edajrenAAnrv"
   },
   "source": [
    "**<font color='teal'> The intercept is the mean `AdultWeekend` price for all the resorts given the other characteristics. The addition or subtraction of each of the coefficient values in the regression are numeric adjustments applied to the intercept to provide a particular observation's value for the resulting `AdultWeekend` value. Also, because we took the time to scale our x values in the training data, we can compare each of the coeeficients for the features to determine the feature importances. Print the coefficient values from the linear model and sort in descending order to identify the top ten most important features.</font>** \n",
    "\n",
    "\n",
    "Hint: make sure to review the absolute value of the coefficients, because the adjustment may be positive or negative, but what we are looking for is the magnitude of impact on our response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEKc_lmZAnrw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>7.072190e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>3.750834e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>3.697047e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>3.153368e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>1.797423e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>1.504784e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_New York</th>\n",
       "      <td>1.067045e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Michigan</th>\n",
       "      <td>9.392632e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_California</th>\n",
       "      <td>8.481420e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_New Hampshire</th>\n",
       "      <td>8.145837e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Pennsylvania</th>\n",
       "      <td>8.145837e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Wisconsin</th>\n",
       "      <td>7.790984e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Vermont</th>\n",
       "      <td>7.011617e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Minnesota</th>\n",
       "      <td>7.011617e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_New Mexico</th>\n",
       "      <td>6.109378e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Idaho</th>\n",
       "      <td>6.109378e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Connecticut</th>\n",
       "      <td>5.593948e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Maine</th>\n",
       "      <td>5.593948e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Wyoming</th>\n",
       "      <td>5.593948e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Massachusetts</th>\n",
       "      <td>5.018427e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Colorado</th>\n",
       "      <td>5.018427e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Ohio</th>\n",
       "      <td>5.018427e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_North Carolina</th>\n",
       "      <td>4.359078e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_West Virginia</th>\n",
       "      <td>4.359078e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_Illinois</th>\n",
       "      <td>4.359078e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Coefficient\n",
       "total_chairs          7.072190e+13\n",
       "double                3.750834e+13\n",
       "surface               3.697047e+13\n",
       "triple                3.153368e+13\n",
       "quad                  1.797423e+13\n",
       "fastQuads             1.504784e+13\n",
       "state_New York        1.067045e+13\n",
       "state_Michigan        9.392632e+12\n",
       "state_California      8.481420e+12\n",
       "state_New Hampshire   8.145837e+12\n",
       "state_Pennsylvania    8.145837e+12\n",
       "state_Wisconsin       7.790984e+12\n",
       "state_Vermont         7.011617e+12\n",
       "state_Minnesota       7.011617e+12\n",
       "state_New Mexico      6.109378e+12\n",
       "state_Idaho           6.109378e+12\n",
       "state_Connecticut     5.593948e+12\n",
       "state_Maine           5.593948e+12\n",
       "state_Wyoming         5.593948e+12\n",
       "state_Massachusetts   5.018427e+12\n",
       "state_Colorado        5.018427e+12\n",
       "state_Ohio            5.018427e+12\n",
       "state_North Carolina  4.359078e+12\n",
       "state_West Virginia   4.359078e+12\n",
       "state_Illinois        4.359078e+12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You might want to make a pandas DataFrame displaying the coefficients for each state like so: \n",
    "# pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])\n",
    "pd.DataFrame(abs(lm.coef_), X1.columns, columns=['Coefficient']).sort_values('Coefficient', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpdALMoAAnry"
   },
   "source": [
    "**<font color='teal'>You should see that the top ten important features are different states. However, the state is not something the managers at the Big Mountain Resort can do anything about. Given that we care more about actionable traits associated with ticket pricing, rebuild the model without the state features and compare the results. </font>**\n",
    "\n",
    "Hint: Try to construct another model using exactly the steps we followed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mHYA1BzAnrz"
   },
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pz1YXAdiAnr0"
   },
   "outputs": [],
   "source": [
    "#all second model set\n",
    "model_x2y2 = lm.fit(X2_train,y_train)\n",
    "#model_x22y22 = lm.fit(X2_train2,y2_train2)\n",
    "#model_x23y23 = lm.fit(X2_train3,y2_train3)\n",
    "#model_x24y24 = lm.fit(X2_train4,y2_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM1EGf16Anr2"
   },
   "outputs": [],
   "source": [
    "y_pred2 = model_x2y2.predict(X2_test)\n",
    "#y_pred22 = model_x22y22.predict(X2_test2)\n",
    "#y_pred23 = model_x23y23.predict(X2_test3)\n",
    "#y_pred24 = model_x24y24.predict(X2_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7871390611216698\n"
     ]
    }
   ],
   "source": [
    "# You might want to use the explained_variance_score() and mean_absolute_error() metrics.\n",
    "# To do so, you will need to import them from sklearn.metrics. \n",
    "# You can plug y_test and y_pred into the functions to evaluate the model\n",
    "print(explained_variance_score(y2_test, y_pred2))\n",
    "#print(explained_variance_score(y2_test2, y_pred22))\n",
    "#print(explained_variance_score(y2_test3, y_pred23))\n",
    "#print(explained_variance_score(y2_test4, y_pred24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.431639109091467\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y2_test, y_pred2))\n",
    "#print(mean_absolute_error(y2_test2, y_pred22))\n",
    "#print(mean_absolute_error(y2_test3, y_pred23))\n",
    "#print(mean_absolute_error(y2_test4, y_pred24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdultWeekday</th>\n",
       "      <td>9.421309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusters</th>\n",
       "      <td>2.999231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <td>2.332554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>2.279237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>1.962653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>1.598580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <td>1.521691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averageSnowfall</th>\n",
       "      <td>1.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>1.341639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snow Making_ac</th>\n",
       "      <td>1.076228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>1.007365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongestRun_mi</th>\n",
       "      <td>0.992196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>0.881646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>0.839465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TerrainParks</th>\n",
       "      <td>0.752590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>0.668707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>0.657203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightSkiing_ac</th>\n",
       "      <td>0.617331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearsOpen</th>\n",
       "      <td>0.452626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0.356856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Coefficient\n",
       "AdultWeekday          9.421309\n",
       "clusters              2.999231\n",
       "daysOpenLastYear      2.332554\n",
       "summit_elev           2.279237\n",
       "vertical_drop         1.962653\n",
       "triple                1.598580\n",
       "projectedDaysOpen     1.521691\n",
       "averageSnowfall       1.501400\n",
       "SkiableTerrain_ac     1.341639\n",
       "Snow Making_ac        1.076228\n",
       "quad                  1.007365\n",
       "LongestRun_mi         0.992196\n",
       "surface               0.881646\n",
       "total_chairs          0.839465\n",
       "TerrainParks          0.752590\n",
       "fastQuads             0.668707\n",
       "Runs                  0.657203\n",
       "NightSkiing_ac        0.617331\n",
       "yearsOpen             0.452626\n",
       "double                0.356856"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(abs(lm.coef_), X2.columns, columns=['Coefficient']).sort_values('Coefficient', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWjQLr3LAnr6"
   },
   "source": [
    "**<font color='teal'> When reviewing our new model coefficients, we see `summit_elev` is now in the number two spot. This is also difficult to change from a management prespective and highly correlated with `base_elev` and `vertical_drop`.  This time, rebuild the model without the state features and without the `summit_elev` and without `base_elev`and compare the results. </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXqvcn93Anr7"
   },
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eugnDNNAnr8"
   },
   "outputs": [],
   "source": [
    "#all third model set\n",
    "model_x3y3 = lm.fit(X3_train,y_train)\n",
    "#model_x32y32 = lm.fit(X3_train2,y3_train2)\n",
    "#model_x33y33 = lm.fit(X3_train3,y3_train3)\n",
    "#model_x34y34 = lm.fit(X3_train4,y3_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pq0pW7G9Anr_"
   },
   "outputs": [],
   "source": [
    "y_pred3 = model_x3y3.predict(X3_test)\n",
    "#y_pred32 = model_x32y32.predict(X3_test2)\n",
    "#y_pred33 = model_x33y33.predict(X3_test3)\n",
    "#y_pred34 = model_x34y34.predict(X3_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804593188560253\n"
     ]
    }
   ],
   "source": [
    "# You might want to use the explained_variance_score() and mean_absolute_error() metrics.\n",
    "# To do so, you will need to import them from sklearn.metrics. \n",
    "# You can plug y_test and y_pred into the functions to evaluate the model\n",
    "print(explained_variance_score(y3_test, y_pred3))\n",
    "#print(explained_variance_score(y3_test2, y_pred32))\n",
    "#print(explained_variance_score(y3_test3, y_pred33))\n",
    "#print(explained_variance_score(y3_test4, y_pred34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.080286590992823\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y3_test, y_pred3))\n",
    "#print(mean_absolute_error(y3_test2, y_pred32))\n",
    "#print(mean_absolute_error(y3_test3, y_pred33))\n",
    "#print(mean_absolute_error(y3_test4, y_pred34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reXlf0HAAnsG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdultWeekday</th>\n",
       "      <td>10.290155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>2.723993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <td>2.168678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <td>2.129318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusters</th>\n",
       "      <td>1.830772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongestRun_mi</th>\n",
       "      <td>1.536929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightSkiing_ac</th>\n",
       "      <td>1.430719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TerrainParks</th>\n",
       "      <td>1.358349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>0.938137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>0.867644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Coefficient\n",
       "AdultWeekday         10.290155\n",
       "SkiableTerrain_ac     2.723993\n",
       "daysOpenLastYear      2.168678\n",
       "projectedDaysOpen     2.129318\n",
       "clusters              1.830772\n",
       "LongestRun_mi         1.536929\n",
       "NightSkiing_ac        1.430719\n",
       "TerrainParks          1.358349\n",
       "vertical_drop         0.938137\n",
       "Runs                  0.867644"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(abs(lm.coef_), X3.columns, columns=['Coefficient']).sort_values('Coefficient', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['summit_elev', 'vertical_drop', 'fastQuads', 'quad', 'triple', 'double',\n",
      "       'surface', 'total_chairs', 'Runs', 'TerrainParks', 'LongestRun_mi',\n",
      "       'SkiableTerrain_ac', 'Snow Making_ac', 'daysOpenLastYear', 'yearsOpen',\n",
      "       'averageSnowfall', 'AdultWeekday', 'projectedDaysOpen',\n",
      "       'NightSkiing_ac', 'clusters', 'state_Alaska', 'state_Arizona',\n",
      "       'state_California', 'state_Colorado', 'state_Connecticut',\n",
      "       'state_Idaho', 'state_Illinois', 'state_Indiana', 'state_Iowa',\n",
      "       'state_Maine', 'state_Maryland', 'state_Massachusetts',\n",
      "       'state_Michigan', 'state_Minnesota', 'state_Missouri', 'state_Montana',\n",
      "       'state_Nevada', 'state_New Hampshire', 'state_New Jersey',\n",
      "       'state_New Mexico', 'state_New York', 'state_North Carolina',\n",
      "       'state_Ohio', 'state_Oregon', 'state_Pennsylvania',\n",
      "       'state_Rhode Island', 'state_South Dakota', 'state_Tennessee',\n",
      "       'state_Utah', 'state_Vermont', 'state_Virginia', 'state_Washington',\n",
      "       'state_West Virginia', 'state_Wisconsin', 'state_Wyoming'],\n",
      "      dtype='object')\n",
      "Index(['summit_elev', 'vertical_drop', 'fastQuads', 'quad', 'triple', 'double',\n",
      "       'surface', 'total_chairs', 'Runs', 'TerrainParks', 'LongestRun_mi',\n",
      "       'SkiableTerrain_ac', 'Snow Making_ac', 'daysOpenLastYear', 'yearsOpen',\n",
      "       'averageSnowfall', 'AdultWeekday', 'projectedDaysOpen',\n",
      "       'NightSkiing_ac', 'clusters'],\n",
      "      dtype='object')\n",
      "Index(['vertical_drop', 'Runs', 'TerrainParks', 'LongestRun_mi',\n",
      "       'SkiableTerrain_ac', 'daysOpenLastYear', 'AdultWeekday',\n",
      "       'projectedDaysOpen', 'NightSkiing_ac', 'clusters'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X1.columns)\n",
    "print(X2.columns)\n",
    "print(X3.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJvQMns6AnsI"
   },
   "source": [
    "## Identify the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LD7-3iLcAnsK"
   },
   "source": [
    "**<font color='teal'> Review the model performances in the table below and choose the best model for proving insights to Big Mountain management about what features are driving ski resort lift ticket prices. Type your choice in the final markdown cell â€” you will discuss this selection more in the next step of the guided casptone. </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to reanalysis in Exploratory Data Analysis (stepthree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La5S9fRPAnsK"
   },
   "source": [
    "| Model | Model Column| Explained Variance| Mean Absolute Error|Features Dropped|\n",
    "| --- |--- |--- | --- | --- |\n",
    "| Model 1. | Adult Weekend |-0.34 | 52.93 |-|\n",
    "|  | Adult Weekday |-0.16 | 60.15 |-|\n",
    "|  | Days Open Last Year |0.41 | 12.12 |-|\n",
    "|  | Projected Days Open |1.0 | 2.35e-14 |-|\n",
    "| Model 2. | Adult Weekend | -0.34|52.93 |'state'|\n",
    "|  | Adult Weekday | -0.16| 60.15|'state'|\n",
    "|  | Days Open Last Year | 0.41| 12.12|'state'|\n",
    "|  | Projected Days Open | 1.0| 2.12e-14|'state'|\n",
    "| Model 3. | Adult Weekend | -0.34 |52.93 |'state', 'summit_elev', 'trams', 'fastEight', 'fastSixes', 'fastQuads','quad','triple','double','surface','total_chairs', 'Snow Making_ac','yearsOpen','averageSnowfall'|\n",
    "|  | Adult Weekday | -0.16 |60.15 |'state', 'summit_elev', 'trams', 'fastEight', 'fastSixes', 'fastQuads','quad','triple','double','surface','total_chairs', 'Snow Making_ac','yearsOpen','averageSnowfall'|\n",
    "|  | Days Open Last Year | 0.41 |12.12 |'state', 'summit_elev', 'trams', 'fastEight', 'fastSixes', 'fastQuads','quad','triple','double','surface','total_chairs', 'Snow Making_ac','yearsOpen','averageSnowfall'|\n",
    "|  | Projected Days Open| 1.0 |1.12e-15 |'state', 'summit_elev', 'trams', 'fastEight', 'fastSixes', 'fastQuads','quad','triple','double','surface','total_chairs', 'Snow Making_ac','yearsOpen','averageSnowfall'|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2c-zn7TAnsL"
   },
   "source": [
    "Initial Model Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgC0eMBrAnsM"
   },
   "source": [
    "The models show that minimal or no change despite removel of features.  This could be explained by the small sample size that resulted from removal of outliers.  Based on this analysis, feature removal has no significant effect.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reanalysis in Exploratory Data Analysis (stepthree) to remove binary features and choosing a single response variable based on the goal of the project (increasing revenue):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Explained Variance| Mean Absolute Error | Features Dropped |\n",
    "| --- |--- | --- | --- |\n",
    "| Model 1. |0.76|5.98|'-' |\n",
    "| Model 2. |0.78|6.43|'state' |\n",
    "| Model 3. |0.80|6.08|'state','summit_elev','fastQuads','quad','triple','double','surface','total_chairs','SnowMaking_ac','yearsOpen','averageSnowfall' |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fianl Model Selection:\n",
    "Model 3 removes values that became binary when outlier data was removed and removes values with minimal affect on the cost of admission.  It also has the highest explained variance and the error measure is low.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.ProfileReport(df1)  #this is a neat package that provides a great initial look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RtEspslPZyGY",
    "s0DokMkAZyGc",
    "2iuitnKcZyHS",
    "iAWQxougZyHW",
    "ThMTimlBZyHZ",
    "QwZ-LkjXZyHt",
    "srtXEA3N4-Y9",
    "ChVreJupZyIA",
    "zDgSSsq1ZyID",
    "I3GYKWfi5Llg",
    "pmMvrhbI-viE",
    "ZXDPkW3UZyIX",
    "Dnc_vHQLZyId",
    "daJxuJ-dZyIg",
    "mAQ-oHiPZyIn",
    "hnGOsp3mZyIp"
   ],
   "name": "GuidedCapstoneStep5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
