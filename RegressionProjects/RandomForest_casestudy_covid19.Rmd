---
title: "RandomForest_casestudy_covid19"
author: "HGRYK"
date: "September 7, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Coronavirus
Coronavirus disease (COVID-19) is an infectious disease caused by a new virus.
The disease causes respiratory illness (like the flu) with symptoms such as a cough, fever, and in more severe cases, difficulty breathing. You can protect yourself by washing your hands frequently, avoiding touching your face, and avoiding close contact (2 meters or 6 feet) with people who are unwell. An outbreak of COVID-19 started in December 2019 and at the time of the creation of this project was continuing to spread throughout the world. Many governments recommended only essential outings to public places and closed most business that do not serve food or sell essential items. An excellent [spatial dashboard](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6) built by Johns Hopkins shows the daily confirmed cases by country. 

This case study was designed to drive home the important role that data science plays in real-world situations like this pandemic. This case study uses the Random Forest Classifier and a dataset from the South Korean cases of COVID-19 provided on [Kaggle](https://www.kaggle.com/kimjihoo/coronavirusdataset) to encourage research on this important topic. The goal of the case study is to build a Random Forest Classifier to predict the 'state' of the patient.


```{r packages}
library(readr)
library(dplyr)
library(tidyr)
library(miscset)

library(tidyverse)
library(tidymodels)
library(reticulate)
library(umap)
reticulate::use_python("C:/Users/gryka/Anaconda3/python.exe", required=TRUE)
#use_condaenv(condaenv='Anaconda3', required = TRUE)
# py_run_string('import umap')
```

```{python packages}
import pandas as pd
import numpy as np
```

```{python}
t = 2+2
print(t)
#test = pandas.read_csv("PatientInfo.csv")
```



Import the data and take a look.  
```{r}
df <- read_csv("PatientInfo.csv")
nrow(df)
ncol(df)
head(df, n = 6)
```

Quickly it becomes clear that there are null values in the dataset that need to be addressed in order to predict the 'state' of the patient.

```{r}
# count null values
na_count <-sapply(df, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count[order(-na_count)])
head(na_count, n=6)
```

The table above shows the counts of null values for each variable.  Those with null values need to be addressed.  Some values can be populated with the mean, some calculated based on filled values, a few date features will be filled with 0 as an indicator that the event has not occured, and others we will use fill() to complete.  Each strategy is annotated in the code blocks below.  

```{r}
#populate null values in numeric columns with mean of column

features = c('global_num', 'birth_year', 'infection_order', 'contact_number')

for(i in features){
  #print(i)
  df[is.na(df[,i]), i] <- sapply(df[i], mean, na.rm=TRUE)
}
```

```{r}
#calculate values based on what we populated
today = as.numeric(format(Sys.Date(), "%Y"))

df$n_age <- today - df$birth_year

#binning years to #s for age
bins <- c(1910, 1920, 1930, 1940,1950,1960,1970,1980,1990,2000, 2010, Inf)
names <- c('100s', '90s', '80s', '70s', '60s', '50s', '40s', '30s', '20s', '10s', '0s')

df$age <- cut(df$birth_year, breaks = bins, labels = names)

```
```{r}
# for disease, replace true with 1 and na/false with 0
df$disease <- lapply(df$disease, as.numeric)
df$disease[is.na(df$disease)] <-0
```

```{r}
# fill in date values that cannot be predicted with 0
df$released_date <- as.character(df$released_date)
df$released_date[is.na(df$released_date)] <- 0

df$deceased_date <- as.character(df$deceased_date)
df$deceased_date[is.na(df$deceased_date)] <- 0
```

```{r}
# fill missing values in confirmed date, then fill missing values in symptom onset with confirmed date
df <- df %>% fill("confirmed_date")

df$symptom_onset_date <- df$symptom_onset_date %>% coalesce(df$symptom_onset_date, df$confirmed_date)
```

```{r}
#fill in infected_by and infection_case
df <- df %>% fill(c("infected_by", "infection_case", "sex", "city"), .direction = "down")
df <- df %>% fill("infected_by", .direction = "up")
```

Now that we have completed all of the imputation, let's check for null values again to make sure we did not miss anything.

```{r}
# check for null values
na_count <-sapply(df, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count[order(-na_count)])
head(na_count, n=6)
```

Note, we would like to predict 'state', so we will avoid filling null values here, but all the predictive features are populated.  

There were many missing values in the date columns which were imputed or set as an indicator value.  These will likely not give us the best information and for the first several months of the pandemic, cases were still low enough that one date vs another wont provide much predictive power.  Let's drop the date columns.
```{r}
df <- subset(df, select = -c(`symptom_onset_date`, `confirmed_date`, `released_date`, `deceased_date`))
```












